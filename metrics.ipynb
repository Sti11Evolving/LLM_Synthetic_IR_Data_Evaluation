{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qrel_reconstruction import *\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "msmarco_passage_v2 = load_dataset(\"data/msmarco-passage-v2-original.csv\", \"msmarco-passage-v2/trec-dl-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([411, 219, 213, 158], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(msmarco_passage_v2.loc[:1000, 'Relevance Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.096056622851364%\n",
      "Binary Accuracy: 72.80080889787665%\n",
      "Average error: 0.7492416582406471\n",
      "Average offset: 0.43174924165824063\n",
      "Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical zeroshot\n",
    "\n",
    "run_name = \"gpt3.5-zeroshot\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'] != -1].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.24266936299292%\n",
      "Binary Accuracy: 68.14964610717897%\n",
      "Average error: 0.8048533872598584\n",
      "Average offset: 0.564206268958544\n",
      "Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical fewshot\n",
    "\n",
    "run_name = \"gpt3.5-fewshot\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'] != -1].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.93794749403341%\n",
      "Binary Accuracy: 83.05489260143199%\n",
      "Average error: 0.6085918854415274\n",
      "Average offset: 0.021479713603818614\n",
      "Cost: 0.004547000000000001\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical reversed\n",
    "\n",
    "run_name = \"gpt3.5-reversed\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'].notnull()].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.39939332659252%\n",
      "Binary Accuracy: 75.12639029322547%\n",
      "Average error: 0.8372093023255814\n",
      "Average offset: 0.6491405460060667\n",
      "Cost: 0.2403795\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical scaled\n",
    "\n",
    "run_name = \"gpt3.5-scaled\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'].notnull()].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "# Rescale into 0-3\n",
    "low = 1\n",
    "high = 10\n",
    "\n",
    "predicted = np.round((predicted-low)*(3/(high-low)))\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.434782608695656%\n",
      "Binary Accuracy: 65.62184024266936%\n",
      "Average error: 1.1749241658240648\n",
      "Average offset: 1.1081900910010112\n",
      "Cost: 0.344403\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical symetrical\n",
    "\n",
    "run_name = \"gpt3.5-symetrical\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'].notnull()].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "# Rescale into 0-3\n",
    "low = -3\n",
    "high = 3\n",
    "\n",
    "predicted = np.round((predicted-low)*(3/(high-low)))\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.950455005055616%\n",
      "Binary Accuracy: 70.07077856420626%\n",
      "Average error: 0.8291203235591507\n",
      "Average offset: 0.5904954499494439\n",
      "Cost: 0.5533605\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical step-by-step\n",
    "\n",
    "run_name = \"gpt3.5-step_by_step\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'] != -1].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 36.5015166835187%\n",
      "Binary Accuracy: 68.95854398382204%\n",
      "Average error: 0.7512639029322548\n",
      "Average offset: 0.32861476238624876\n",
      "Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics for numerical phrase based\n",
    "\n",
    "run_name = \"gpt3.5-zeroshot-phrase\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'] != -1].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'])\n",
    "\n",
    "# map phrases to numeric values\n",
    "phrase_to_score = {\n",
    "    \"Irrelevant\":0,\n",
    "    \"Related\":1,\n",
    "    \"Highly\":2,\n",
    "    \"Perfectly\":3,\n",
    "}\n",
    "\n",
    "predicted = np.array(list(map(lambda x: phrase_to_score[x], predicted)))\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Accuracy: {100*(np.average(actual==predicted))}%')\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy: 76.54196157735086%\n",
      "Average error: 0.7623862487360971\n",
      "Average offset: -0.3619817997977755\n",
      "Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics for simple Y/N\n",
    "\n",
    "run_name = \"gpt3.5-zeroshot-yn\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'] != -1].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'], dtype=int)\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted_binary))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Accuracy: 73.88663967611336%\n",
      "Average error: 0.7894736842105263\n",
      "Average offset: -0.32186234817813764\n",
      "Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics for explained Y/N\n",
    "\n",
    "run_name = \"gpt3.5-zeroshot-yn-explained\"\n",
    "experiment = load_experiments(f\"data/{run_name}.csv\", msmarco_passage_v2)\n",
    "\n",
    "# Remove unpredicted qrels\n",
    "num_predictions = 1000\n",
    "experiment = experiment.loc[experiment['Relevance Predicted'].notnull()].loc[:num_predictions]\n",
    "\n",
    "actual = np.array(experiment['Relevance Actual'])\n",
    "predicted = np.array(experiment['Relevance Predicted'], dtype=int)\n",
    "\n",
    "actual_binary = np.array(actual, dtype=bool)\n",
    "predicted_binary = np.array(predicted, dtype=bool)\n",
    "\n",
    "print(f'Binary Accuracy: {100*(np.average(actual_binary==predicted))}%')\n",
    "\n",
    "print(f'Average error: {np.average(np.abs(actual-predicted))}')\n",
    "print(f'Average offset: {np.average(predicted-actual)}')\n",
    "\n",
    "\n",
    "prompt_tokens = int(os.getenv(f\"{run_name}_prompt_tokens\", 0))\n",
    "completion_tokens = int(os.getenv(f\"{run_name}_completion_tokens\", 0))\n",
    "\n",
    "cost = 0.0015*(prompt_tokens/1000) + 0.002*(completion_tokens/1000)\n",
    "\n",
    "print(f\"Cost: {cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
