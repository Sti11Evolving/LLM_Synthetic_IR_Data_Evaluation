{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyserini'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyserini\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlucene\u001b[39;00m \u001b[39mimport\u001b[39;00m LuceneSearcher\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mir_measures\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mir_measures\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyserini'"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from ir_measures import *\n",
    "import ir_measures\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full qrels dataset\n",
    "reconsruction_dataset = pd.read_csv(\n",
    "    \"data/msmarco-passage-v2-reconstruction.csv\", index_col=0\n",
    ")\n",
    "\n",
    "# Extract the queries\n",
    "queries = reconsruction_dataset.loc[\n",
    "    reconsruction_dataset.loc[:, \"Query\"].drop_duplicates().index, [\"QueryID\", \"Query\"]\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retrieves the scores of the top 1000 hit for each query given a paritcular Lucene index.\n",
    "'''\n",
    "def get_lucene_search_results(index: str) -> pd.DataFrame:\n",
    "    results = []\n",
    "    searcher = LuceneSearcher.from_prebuilt_index(index)\n",
    "\n",
    "    # For each query\n",
    "    for query_id, query in tqdm(queries.values):\n",
    "        # Get the top 1000 hits\n",
    "        for hit in searcher.search(query, k=1000):\n",
    "            # Add the hit to the list\n",
    "            results.append([query_id, hit.docid, hit.score])\n",
    "\n",
    "    # Convert list to dataframe\n",
    "    return pd.DataFrame(results, columns=[\"QueryID\", \"DocumentID\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_results = get_lucene_search_results(\"msmarco-v2-passage\")\n",
    "BM25_results.to_csv(\"data/BM25_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_Augmented_results = get_lucene_search_results(\"msmarco-v2-passage-augmented\")\n",
    "BM25_Augmented_results.to_csv(\"data/BM25_Augmented_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_d2q_t5_results = get_lucene_search_results(\"msmarco-v2-passage-d2q-t5\")\n",
    "BM25_d2q_t5_results.to_csv(\"data/BM25_d2q_t5_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_Augmented_d2q_t5_results = get_lucene_search_results(\"msmarco-v2-passage-augmented-d2q-t5\")\n",
    "BM25_Augmented_d2q_t5_results.to_csv(\"data/BM25_Augmented_d2q_t5_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniCOIL_results = get_lucene_search_results(\"msmarco-v2-passage-unicoil-0shot\")\n",
    "uniCOIL_results.to_csv(\"data/uniCOIL_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat all of the data into a format that ir_measures can use\n",
    "\n",
    "def to_ir_measures_qrel_format(dataset: pd.DataFrame, qrel_col: str) -> pd.DataFrame:\n",
    "    qrels = \"\"\n",
    "    for query_id, doc_id, qrel in dataset.loc[dataset[qrel_col] > 0, [\"QueryID\", \"DocumentID\", qrel_col]].values:\n",
    "        qrels += f\"{query_id} 0 {doc_id} {int(qrel)}\\n\"\n",
    "    return list(ir_measures.read_trec_qrels(qrels))\n",
    "\n",
    "\n",
    "def to_ir_measures_run_format(results: pd.DataFrame) -> pd.DataFrame:\n",
    "    qrels = \"\"\n",
    "    for query_id, doc_id, score in results.values:\n",
    "        qrels += f\"{query_id} 0 {doc_id} 0 {score} 0\\n\"\n",
    "    return list(ir_measures.read_trec_run(qrels))\n",
    "\n",
    "original_qrels = to_ir_measures_qrel_format(reconsruction_dataset, \"Actual\")\n",
    "zeroshot_qrels = to_ir_measures_qrel_format(reconsruction_dataset, \"Zeroshot\")\n",
    "zeroshot_reversed_qrels = to_ir_measures_qrel_format(reconsruction_dataset, \"Zeroshot Reversed\")\n",
    "fewshot_qrels = to_ir_measures_qrel_format(reconsruction_dataset, \"Fewshot\")\n",
    "fewshot_reversed_qrels = to_ir_measures_qrel_format(reconsruction_dataset, \"Fewshot Reversed\")\n",
    "\n",
    "\n",
    "BM25_scores = to_ir_measures_run_format(BM25_results)\n",
    "uniCOIL_scores = to_ir_measures_run_format(reconsruction_dataset, \"uniCOIL zeroshot Search Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n",
      "skipped ['gm_map']: measures not yet supported\n"
     ]
    }
   ],
   "source": [
    "def evaluate_run(qrels, run, qrels_name, run_name):\n",
    "    evaluator = ir_measures.evaluator([AP, nDCG@10, RR@100, R@100, R@1000], qrels)\n",
    "    evaluation = pd.Series(dict(((str(measure), value) for measure, value in evaluator.calc_aggregate(run).items())))\n",
    "    evaluation[\"qrel source\"] = qrels_name\n",
    "    evaluation[\"run\"] = run_name\n",
    "    return evaluation\n",
    "\n",
    "qrels_list = [(original_qrels, \"Actual\"), (zeroshot_qrels, \"Zeroshot\"), (zeroshot_reversed_qrels, \"Zeroshot Reversed\"), (fewshot_qrels, \"Fewshot\"), (fewshot_reversed_qrels, \"Fewshot Reversed\")]\n",
    "runs_list = [(BM25_scores, \"BM25 Search Score\"), (uniCOIL_scores, \"uniCOIL zeroshot Search Score\")]\n",
    "\n",
    "evaluations = []\n",
    "for qrels, qrels_name in qrels_list:\n",
    "    for run, run_name in runs_list:\n",
    "        evaluations.append(evaluate_run(qrels, run, qrels_name, run_name))\n",
    "\n",
    "evaluations = pd.DataFrame(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations.to_csv(\"data/evaluations_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
